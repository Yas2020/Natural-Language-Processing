{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Recognize named entities on Twitter with RNNs\n",
    "\n",
    "In this assignment, we will use a recurrent neural network to solve Named Entity Recognition (NER) problem. NER is a common task in natural language processing systems. It serves for extraction such entities from the text as persons, organizations, locations, etc. In this task, we will experiment to recognize named entities from Twitter.\n",
    "\n",
    "For example, we want to extract persons' and organizations' names from the text. For example, for the input text:\n",
    "\n",
    "    Ian Goodfellow works for Google Brain\n",
    "\n",
    "a NER model needs to provide the following sequence of tags:\n",
    "\n",
    "    B-PER I-PER    O     O   B-ORG  I-ORG\n",
    "\n",
    "Where *B-* and *I-* prefixes stand for the beginning and inside of the entity, while *O* stands for out of tag or no tag. Markup with the prefix scheme is called *BIO markup*. This markup is introduced for distinguishing of consequent entities with similar types.\n",
    "\n",
    "A solution of the task will be based on neural networks, particularly, on Bi-Directional RNN Networks (such as Bi-LSTMs or Bi-GRUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Twitter Named Entity Recognition corpus\n",
    "\n",
    "We will work with a corpus, which contains tweets with NE tags. Every line of a file contains a pair of a token (word/punctuation symbol) and a tag, separated by a whitespace. Different tweets are separated by an empty line.\n",
    "\n",
    "The function *read_data* reads a corpus from the *file_path* and returns two lists: one with tokens and one with the corresponding tags. You need to complete this function by adding a code, which will replace a user's nickname to `<USR>` token and any URL to `<URL>` token. You could think that a URL and a nickname are just strings which start with *http://* or *https://* in case of URLs and a *@* symbol for nicknames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    tweet_tokens = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tweet_tokens:\n",
    "                tokens.append(tweet_tokens)\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_tokens = []\n",
    "            tweet_tags = []\n",
    "        else:\n",
    "            token, tag = line.split()\n",
    "            if '@' in token: token = '<USR>'\n",
    "            if 'http://' in token or 'https://' in token: token = '<URL>'\n",
    "            \n",
    "            # Replace all urls with <URL> token\n",
    "            # Replace all users with <USR> token\n",
    "\n",
    "            tweet_tokens.append(token)\n",
    "            tweet_tags.append(tag)\n",
    "            \n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can load three separate parts of the dataset:\n",
    " - *train* data for training the model;\n",
    " - *validation* data for evaluation and hyperparameters tuning;\n",
    " - *test* data for final evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_tags = read_data('data/train.txt')\n",
    "validation_tokens, validation_tags = read_data('data/validation.txt')\n",
    "test_tokens, test_tags = read_data('data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_tokens` and `train_tags` are lists of lists of tokens and tags respectively, for each sentence. We can print the data running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT\tO\n",
      "<USR>\tO\n",
      ":\tO\n",
      "Online\tO\n",
      "ticket\tO\n",
      "sales\tO\n",
      "for\tO\n",
      "Ghostland\tB-musicartist\n",
      "Observatory\tI-musicartist\n",
      "extended\tO\n",
      "until\tO\n",
      "6\tO\n",
      "PM\tO\n",
      "EST\tO\n",
      "due\tO\n",
      "to\tO\n",
      "high\tO\n",
      "demand\tO\n",
      ".\tO\n",
      "Get\tO\n",
      "them\tO\n",
      "before\tO\n",
      "they\tO\n",
      "sell\tO\n",
      "out\tO\n",
      "...\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
    "        print('%s\\t%s' % (token, tag))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5795"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tokens) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dictionaries\n",
    "\n",
    "To train a neural network, we will use two mappings: \n",
    "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
    "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
    "\n",
    "We implement the function *build_dict* which will return {token or tag}$\\to${index} and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(tokens_or_tags, special_tokens):\n",
    "    \"\"\"\n",
    "        tokens_or_tags: a list of lists of tokens or tags\n",
    "        special_tokens: some special tokens\n",
    "    \"\"\"\n",
    "\n",
    "    tokens_or_tags_flat_ = list(set([x for y in tokens_or_tags for x in y]))\n",
    "    tokens_or_tags_flat = [x for x in tokens_or_tags_flat_ if x not in special_tokens]\n",
    "    tokens_or_tags_flat = special_tokens + tokens_or_tags_flat\n",
    "        \n",
    "    idx2tok = dict(enumerate(tokens_or_tags_flat))\n",
    "    tok2idx = {t:i for i, t in idx2tok.items()}\n",
    "\n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the function *build_dict* you can make dictionaries for tokens and tags. Special tokens in our case will be:\n",
    " - `<UNK>` token for out of vocabulary tokens;\n",
    " - `<PAD>` token for padding sentence to the same length when we create batches of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<PAD>', '<UNK>']\n",
    "special_tags = ['O']\n",
    "\n",
    "# Create dictionaries from training and validation tokens \n",
    "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20458"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token2idx) # total of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the indices of our special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx['<PAD>'], token2idx['<UNK>'], tag2idx['O']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next additional functions will help create the mapping between tokens and ids for sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2idxs(tokens_list):\n",
    "    return [token2idx[word] for word in tokens_list]\n",
    "\n",
    "def tags2idxs(tags_list):\n",
    "    return [tag2idx[tag] for tag in tags_list]\n",
    "\n",
    "def idxs2words(idxs):\n",
    "    return [idx2token[idx] for idx in idxs]\n",
    "\n",
    "def idxs2tags(idxs):\n",
    "    return [idx2tag[idx] for idx in idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing test_tokens and test_tags by replacing out-of-vocabulary tokens by token  `<UNK>` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_tokens)):\n",
    "    for j in range(len(test_tokens[i])):\n",
    "        if test_tokens[i][j] not in token2idx.keys():\n",
    "            test_tokens[i][j] = '<UNK>'\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check the test_tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man\tO\n",
      "i\tO\n",
      "hate\tO\n",
      "when\tO\n",
      "people\tO\n",
      "<UNK>\tO\n",
      "<UNK>\tO\n",
      "luggage\tO\n",
      "..\tO\n",
      "ima\tO\n",
      "just\tO\n",
      "rip\tO\n",
      "it\tO\n",
      "up\tO\n",
      "more\tO\n",
      "with\tO\n",
      "the\tO\n",
      "<UNK>\tO\n",
      "<UNK>\tO\n",
      "<UNK>\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    for token, tag in zip(test_tokens[i], test_tags[i]):\n",
    "        print('%s\\t%s' % (token, tag))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate batches\n",
    "\n",
    "Neural Networks are usually trained with batches. It means that weight updates of the network are based on several sequences at every single time. _The tricky part is that all sequences within a batch need to have the same length_. So we will pad them with a special `<PAD>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_generator(batch_size, \n",
    "                      tokens, tags,\n",
    "                      shuffle=True, \n",
    "                      allow_smaller_last_batch=True):\n",
    "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
    "    \n",
    "    n_samples = len(tokens)\n",
    "    if shuffle:\n",
    "        order = np.random.permutation(n_samples)\n",
    "    else:\n",
    "        order = np.arange(n_samples)\n",
    "\n",
    "    n_batches = n_samples // batch_size\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        max_len_token = 0\n",
    "        for idx in order[batch_start: batch_end]:\n",
    "            x_list.append(words2idxs(tokens[idx]))\n",
    "            y_list.append(tags2idxs(tags[idx]))\n",
    "            max_len_token = max(max_len_token, len(tags[idx]))\n",
    "\n",
    "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
    "        x = np.zeros([current_batch_size, max_len_token], dtype=np.int32) \n",
    "        y = np.zeros([current_batch_size, max_len_token], dtype=np.int32) \n",
    "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
    "        for n in range(current_batch_size):\n",
    "            utt_len = len(x_list[n])\n",
    "            x[n, :utt_len] = x_list[n]\n",
    "            lengths[n] = utt_len\n",
    "            y[n, :utt_len] = y_list[n]\n",
    "        yield x, y \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a recurrent neural network\n",
    "\n",
    "This is the most important part of the assignment. Here we will specify the network architecture based on TensorFlow building blocks. We will create an GRU (LSTM) network which will produce probability distribution over tags for each token in a sentence. To take into account both right and left contexts of the token, we will a Bi-Directional wrapper. Both LSTM and GRU have numerous  parameters, in addition to different implemetations. In particular, we use GRUCell (or could be LSTMCell) looped by RNN layer. The ussual alternative is to use GRU (not its cell) since it does the loop automatically. These different options may result in a slightly different performance depending on the particulat task in hand. Dense layer will be used on top to perform the tag classification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, let us specify the layers of the neural network. First, we need to perform some preparatory steps: \n",
    " \n",
    "- We use an Embedding layer which automatically passes the masking tensor to RNN layer. The mask will help RNN igonre the padding timesteps. This mask should also be used to mask loss terms corresponding to paddings. Instead of using an Embedding layer, we could also initialize a random embeddings matrix and look up the input data from it (look up the input indices in the embedding table using `tf.nn.embedding_lookup`).\n",
    "- Bidirectional wrapper operates forward and backward cells independently. Also, we use dropout as an important regularization technique for neural networks for our task here, which helps us control overfitting.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model with the following parameters:\n",
    " - *vocabulary_size* — number of tokens;\n",
    " - *n_tags* — number of tags;\n",
    " - *embedding_dim* — dimension of embeddings, recommended value: 200;\n",
    " - *n_hidden_rnn* — size of hidden layers for RNN, recommended value: 200;\n",
    " - *PAD_index* — an index of the padding token (`<PAD>`).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 20458 \n",
    "n_tags = 21 \n",
    "embedding_dim = 200 \n",
    "n_hidden = 300 \n",
    "PAD_index = 0\n",
    "batch_size = 32 \n",
    "learning_rate = 0.005 \n",
    "learning_rate_decay = 1.4 \n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "forward_layer = L.RNN(L.GRUCell(n_hidden,dropout=dropout), \n",
    "                     return_sequences=True, \n",
    "                     )\n",
    "\n",
    "backward_layer = L.RNN(L.GRUCell(n_hidden,dropout=dropout), \n",
    "                      return_sequences=True, \n",
    "                      go_backwards=True\n",
    "                      )\n",
    "\n",
    "model.add(L.Embedding(vocabulary_size, embedding_dim, mask_zero=True))\n",
    "model.add(L.Bidirectional(forward_layer, backward_layer=backward_layer))\n",
    "model.add(L.Dense(n_tags))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training we do not need predictions of the network, but we need a loss function. We will use cross-entropy loss efficiently implemented in TF as [cross entropy with logits]. It should be applied to logits of the model (not to softmax probabilities!). Also note, that we do not want to take into account loss terms coming from <PAD> tokens. So we need to mask them out, before computing mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training... \n",
      "\n",
      "Epoch 0/10 \t Trainnig Loss:39.618526458740234\n",
      "Epoch 1/10 \t Trainnig Loss:15.92908763885498\n",
      "Epoch 2/10 \t Trainnig Loss:7.241702556610107\n",
      "Epoch 3/10 \t Trainnig Loss:3.4963269233703613\n",
      "Epoch 4/10 \t Trainnig Loss:1.9717960357666016\n",
      "Epoch 5/10 \t Trainnig Loss:1.3010663986206055\n",
      "Epoch 6/10 \t Trainnig Loss:0.8103141188621521\n",
      "Epoch 7/10 \t Trainnig Loss:0.6042543053627014\n",
      "Epoch 8/10 \t Trainnig Loss:0.4550827741622925\n",
      "Epoch 9/10 \t Trainnig Loss:0.4174806475639343\n",
      "...training finished.\n"
     ]
    }
   ],
   "source": [
    "clip_norm = tf.cast(1.0, tf.float32)\n",
    "\n",
    "train_loss_results = []\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "\n",
    "n_epochs = 10 \n",
    "\n",
    "print('Start training... \\n')\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_loss=0\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "\n",
    "    #Train the model\n",
    "    for x, y in batches_generator(batch_size, train_tokens, train_tags):\n",
    "        \n",
    "        y = tf.one_hot(y, n_tags)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x, training=True)\n",
    "            loss_ = tf.nn.softmax_cross_entropy_with_logits(y,y_pred)\n",
    "            loss = tf.reduce_mean(loss_ * tf.cast( x != 0, dtype=tf.float32))\n",
    " \n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        grads = [tf.clip_by_norm(grad, clip_norm) for grad in grads]\n",
    "    \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        train_loss += loss\n",
    "    \n",
    "    learning_rate = learning_rate/learning_rate_decay\n",
    "    train_loss_results.append(keras.backend.eval(train_loss))\n",
    "    print('Epoch {}/{} \\t Trainnig Loss:{}'.format(epoch, n_epochs, keras.backend.eval(train_loss)))\n",
    "\n",
    "print('...training finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the evaluation process, two functions are provided:\n",
    "predict_tags: uses a model to get predictions and transforms indices to tokens and tags;\n",
    "eval_conll: calculates precision, recall and F1 for the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import precision_recall_f1\n",
    "\n",
    "def predict_tags(model, x_batch):\n",
    "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
    "    \n",
    "    logits = model(x_batch, training=False)\n",
    "    softmax_output = tf.nn.softmax(logits, axis=-1)\n",
    "    tag_idxs_batch = tf.math.argmax(softmax_output,output_type=tf.dtypes.int32, axis=-1)\n",
    "    tag_idxs_batch = keras.backend.eval(tag_idxs_batch)\n",
    "    \n",
    "    tags_batch, tokens_batch = [], []\n",
    "    for tag_idxs, token_idxs in zip(tag_idxs_batch, x_batch):\n",
    "        tags, tokens = [], []\n",
    "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
    "            tags.append(idx2tag[tag_idx])\n",
    "            tokens.append(idx2token[token_idx])\n",
    "        tags_batch.append(tags)\n",
    "        tokens_batch.append(tokens)\n",
    "    return tags_batch, tokens_batch\n",
    "        \n",
    "def eval_conll(model, tokens, tags, short_report=True):\n",
    "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    for x_batch, y_batch in batches_generator(1, tokens, tags):\n",
    "        tags_batch, tokens_batch = predict_tags(model, x_batch)\n",
    "        if len(x_batch[0]) != len(tags_batch[0]):\n",
    "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
    "                            \"expected length: %i, got: %i\" % (len(x_batch[0]), len(tags_batch[0])))\n",
    "        predicted_tags = []\n",
    "        ground_truth_tags = []\n",
    "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0], tokens_batch[0]): \n",
    "            if token != '<PAD>':\n",
    "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
    "                predicted_tags.append(pred_tag)\n",
    "\n",
    "        # We extend every prediction and ground truth sequence with 'O' tag\n",
    "        # to indicate a possible end of entity.\n",
    "        y_true.extend(ground_truth_tags + ['O'])\n",
    "        y_pred.extend(predicted_tags + ['O'])\n",
    "        \n",
    "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4501 phrases; correct: 4467.\n",
      "\n",
      "precision:  99.24%; recall:  99.51%; F1:  99.38\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 423 phrases; correct: 208.\n",
      "\n",
      "precision:  49.17%; recall:  38.73%; F1:  43.33\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('company',\n",
       "              OrderedDict([('precision', 67.44186046511628),\n",
       "                           ('recall', 55.769230769230774),\n",
       "                           ('f1', 61.05263157894737),\n",
       "                           ('n_predicted_entities', 86),\n",
       "                           ('n_true_entities', 104)])),\n",
       "             ('facility',\n",
       "              OrderedDict([('precision', 48.57142857142857),\n",
       "                           ('recall', 50.0),\n",
       "                           ('f1', 49.27536231884058),\n",
       "                           ('n_predicted_entities', 35),\n",
       "                           ('n_true_entities', 34)])),\n",
       "             ('geo-loc',\n",
       "              OrderedDict([('precision', 69.66292134831461),\n",
       "                           ('recall', 54.86725663716814),\n",
       "                           ('f1', 61.38613861386139),\n",
       "                           ('n_predicted_entities', 89),\n",
       "                           ('n_true_entities', 113)])),\n",
       "             ('movie',\n",
       "              OrderedDict([('precision', 0.0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 2),\n",
       "                           ('n_true_entities', 7)])),\n",
       "             ('musicartist',\n",
       "              OrderedDict([('precision', 31.25),\n",
       "                           ('recall', 17.857142857142858),\n",
       "                           ('f1', 22.727272727272727),\n",
       "                           ('n_predicted_entities', 16),\n",
       "                           ('n_true_entities', 28)])),\n",
       "             ('other',\n",
       "              OrderedDict([('precision', 28.04878048780488),\n",
       "                           ('recall', 28.39506172839506),\n",
       "                           ('f1', 28.220858895705522),\n",
       "                           ('n_predicted_entities', 82),\n",
       "                           ('n_true_entities', 81)])),\n",
       "             ('person',\n",
       "              OrderedDict([('precision', 45.20547945205479),\n",
       "                           ('recall', 29.464285714285715),\n",
       "                           ('f1', 35.67567567567567),\n",
       "                           ('n_predicted_entities', 73),\n",
       "                           ('n_true_entities', 112)])),\n",
       "             ('product',\n",
       "              OrderedDict([('precision', 14.285714285714285),\n",
       "                           ('recall', 8.823529411764707),\n",
       "                           ('f1', 10.90909090909091),\n",
       "                           ('n_predicted_entities', 21),\n",
       "                           ('n_true_entities', 34)])),\n",
       "             ('sportsteam',\n",
       "              OrderedDict([('precision', 53.84615384615385),\n",
       "                           ('recall', 35.0),\n",
       "                           ('f1', 42.42424242424242),\n",
       "                           ('n_predicted_entities', 13),\n",
       "                           ('n_true_entities', 20)])),\n",
       "             ('tvshow',\n",
       "              OrderedDict([('precision', 0.0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 6),\n",
       "                           ('n_true_entities', 4)]))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train data evaluation:')\n",
    "eval_conll(model, train_tokens, train_tags, short_report=True)\n",
    "print('Validation data evaluation:')\n",
    "eval_conll(model, validation_tokens, validation_tags, short_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see full quality reports for the final model on train, validation, and test sets. You could expect F-score about 40% on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Train set quality: --------------------\n",
      "processed 105778 tokens with 4489 phrases; found: 4501 phrases; correct: 4467.\n",
      "\n",
      "precision:  99.24%; recall:  99.51%; F1:  99.38\n",
      "\n",
      "\t     company: precision:   99.38%; recall:   99.38%; F1:   99.38; predicted:   643\n",
      "\n",
      "\t    facility: precision:   97.48%; recall:   98.41%; F1:   97.94; predicted:   317\n",
      "\n",
      "\t     geo-loc: precision:   99.80%; recall:   99.80%; F1:   99.80; predicted:   996\n",
      "\n",
      "\t       movie: precision:  100.00%; recall:  100.00%; F1:  100.00; predicted:    68\n",
      "\n",
      "\t musicartist: precision:   99.14%; recall:   99.57%; F1:   99.35; predicted:   233\n",
      "\n",
      "\t       other: precision:   98.43%; recall:   99.60%; F1:   99.02; predicted:   766\n",
      "\n",
      "\t      person: precision:   99.89%; recall:   99.77%; F1:   99.83; predicted:   885\n",
      "\n",
      "\t     product: precision:   99.68%; recall:   99.37%; F1:   99.53; predicted:   317\n",
      "\n",
      "\t  sportsteam: precision:   99.54%; recall:   99.08%; F1:   99.31; predicted:   216\n",
      "\n",
      "\t      tvshow: precision:   95.00%; recall:   98.28%; F1:   96.61; predicted:    60\n",
      "\n",
      "-------------------- Validation set quality: --------------------\n",
      "processed 12836 tokens with 537 phrases; found: 423 phrases; correct: 208.\n",
      "\n",
      "precision:  49.17%; recall:  38.73%; F1:  43.33\n",
      "\n",
      "\t     company: precision:   67.44%; recall:   55.77%; F1:   61.05; predicted:    86\n",
      "\n",
      "\t    facility: precision:   48.57%; recall:   50.00%; F1:   49.28; predicted:    35\n",
      "\n",
      "\t     geo-loc: precision:   69.66%; recall:   54.87%; F1:   61.39; predicted:    89\n",
      "\n",
      "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     2\n",
      "\n",
      "\t musicartist: precision:   31.25%; recall:   17.86%; F1:   22.73; predicted:    16\n",
      "\n",
      "\t       other: precision:   28.05%; recall:   28.40%; F1:   28.22; predicted:    82\n",
      "\n",
      "\t      person: precision:   45.21%; recall:   29.46%; F1:   35.68; predicted:    73\n",
      "\n",
      "\t     product: precision:   14.29%; recall:    8.82%; F1:   10.91; predicted:    21\n",
      "\n",
      "\t  sportsteam: precision:   53.85%; recall:   35.00%; F1:   42.42; predicted:    13\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     6\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('company',\n",
       "              OrderedDict([('precision', 67.44186046511628),\n",
       "                           ('recall', 55.769230769230774),\n",
       "                           ('f1', 61.05263157894737),\n",
       "                           ('n_predicted_entities', 86),\n",
       "                           ('n_true_entities', 104)])),\n",
       "             ('facility',\n",
       "              OrderedDict([('precision', 48.57142857142857),\n",
       "                           ('recall', 50.0),\n",
       "                           ('f1', 49.27536231884058),\n",
       "                           ('n_predicted_entities', 35),\n",
       "                           ('n_true_entities', 34)])),\n",
       "             ('geo-loc',\n",
       "              OrderedDict([('precision', 69.66292134831461),\n",
       "                           ('recall', 54.86725663716814),\n",
       "                           ('f1', 61.38613861386139),\n",
       "                           ('n_predicted_entities', 89),\n",
       "                           ('n_true_entities', 113)])),\n",
       "             ('movie',\n",
       "              OrderedDict([('precision', 0.0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 2),\n",
       "                           ('n_true_entities', 7)])),\n",
       "             ('musicartist',\n",
       "              OrderedDict([('precision', 31.25),\n",
       "                           ('recall', 17.857142857142858),\n",
       "                           ('f1', 22.727272727272727),\n",
       "                           ('n_predicted_entities', 16),\n",
       "                           ('n_true_entities', 28)])),\n",
       "             ('other',\n",
       "              OrderedDict([('precision', 28.04878048780488),\n",
       "                           ('recall', 28.39506172839506),\n",
       "                           ('f1', 28.220858895705522),\n",
       "                           ('n_predicted_entities', 82),\n",
       "                           ('n_true_entities', 81)])),\n",
       "             ('person',\n",
       "              OrderedDict([('precision', 45.20547945205479),\n",
       "                           ('recall', 29.464285714285715),\n",
       "                           ('f1', 35.67567567567567),\n",
       "                           ('n_predicted_entities', 73),\n",
       "                           ('n_true_entities', 112)])),\n",
       "             ('product',\n",
       "              OrderedDict([('precision', 14.285714285714285),\n",
       "                           ('recall', 8.823529411764707),\n",
       "                           ('f1', 10.90909090909091),\n",
       "                           ('n_predicted_entities', 21),\n",
       "                           ('n_true_entities', 34)])),\n",
       "             ('sportsteam',\n",
       "              OrderedDict([('precision', 53.84615384615385),\n",
       "                           ('recall', 35.0),\n",
       "                           ('f1', 42.42424242424242),\n",
       "                           ('n_predicted_entities', 13),\n",
       "                           ('n_true_entities', 20)])),\n",
       "             ('tvshow',\n",
       "              OrderedDict([('precision', 0.0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 6),\n",
       "                           ('n_true_entities', 4)]))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
    "eval_conll(model, train_tokens, train_tags, short_report=False)\n",
    "print('-' * 20 + ' Validation set quality: ' + '-' * 20)\n",
    "eval_conll(model, validation_tokens, validation_tags, short_report=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Test set quality: --------------------\n",
      "processed 13258 tokens with 604 phrases; found: 524 phrases; correct: 249.\n",
      "\n",
      "precision:  47.52%; recall:  41.23%; F1:  44.15\n",
      "\n",
      "\t     company: precision:   62.71%; recall:   44.05%; F1:   51.75; predicted:    59\n",
      "\n",
      "\t    facility: precision:   36.07%; recall:   46.81%; F1:   40.74; predicted:    61\n",
      "\n",
      "\t     geo-loc: precision:   75.61%; recall:   56.36%; F1:   64.58; predicted:   123\n",
      "\n",
      "\t       movie: precision:   16.67%; recall:   12.50%; F1:   14.29; predicted:     6\n",
      "\n",
      "\t musicartist: precision:   10.00%; recall:   11.11%; F1:   10.53; predicted:    30\n",
      "\n",
      "\t       other: precision:   33.33%; recall:   35.92%; F1:   34.58; predicted:   111\n",
      "\n",
      "\t      person: precision:   56.25%; recall:   43.27%; F1:   48.91; predicted:    80\n",
      "\n",
      "\t     product: precision:   14.81%; recall:   14.29%; F1:   14.55; predicted:    27\n",
      "\n",
      "\t  sportsteam: precision:   26.92%; recall:   22.58%; F1:   24.56; predicted:    26\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('company',\n",
       "              OrderedDict([('precision', 62.71186440677966),\n",
       "                           ('recall', 44.047619047619044),\n",
       "                           ('f1', 51.74825174825175),\n",
       "                           ('n_predicted_entities', 59),\n",
       "                           ('n_true_entities', 84)])),\n",
       "             ('facility',\n",
       "              OrderedDict([('precision', 36.0655737704918),\n",
       "                           ('recall', 46.808510638297875),\n",
       "                           ('f1', 40.74074074074074),\n",
       "                           ('n_predicted_entities', 61),\n",
       "                           ('n_true_entities', 47)])),\n",
       "             ('geo-loc',\n",
       "              OrderedDict([('precision', 75.60975609756098),\n",
       "                           ('recall', 56.36363636363636),\n",
       "                           ('f1', 64.58333333333333),\n",
       "                           ('n_predicted_entities', 123),\n",
       "                           ('n_true_entities', 165)])),\n",
       "             ('movie',\n",
       "              OrderedDict([('precision', 16.666666666666664),\n",
       "                           ('recall', 12.5),\n",
       "                           ('f1', 14.285714285714285),\n",
       "                           ('n_predicted_entities', 6),\n",
       "                           ('n_true_entities', 8)])),\n",
       "             ('musicartist',\n",
       "              OrderedDict([('precision', 10.0),\n",
       "                           ('recall', 11.11111111111111),\n",
       "                           ('f1', 10.526315789473685),\n",
       "                           ('n_predicted_entities', 30),\n",
       "                           ('n_true_entities', 27)])),\n",
       "             ('other',\n",
       "              OrderedDict([('precision', 33.33333333333333),\n",
       "                           ('recall', 35.92233009708738),\n",
       "                           ('f1', 34.57943925233645),\n",
       "                           ('n_predicted_entities', 111),\n",
       "                           ('n_true_entities', 103)])),\n",
       "             ('person',\n",
       "              OrderedDict([('precision', 56.25),\n",
       "                           ('recall', 43.269230769230774),\n",
       "                           ('f1', 48.91304347826087),\n",
       "                           ('n_predicted_entities', 80),\n",
       "                           ('n_true_entities', 104)])),\n",
       "             ('product',\n",
       "              OrderedDict([('precision', 14.814814814814813),\n",
       "                           ('recall', 14.285714285714285),\n",
       "                           ('f1', 14.545454545454543),\n",
       "                           ('n_predicted_entities', 27),\n",
       "                           ('n_true_entities', 28)])),\n",
       "             ('sportsteam',\n",
       "              OrderedDict([('precision', 26.923076923076923),\n",
       "                           ('recall', 22.58064516129032),\n",
       "                           ('f1', 24.56140350877193),\n",
       "                           ('n_predicted_entities', 26),\n",
       "                           ('n_true_entities', 31)])),\n",
       "             ('tvshow',\n",
       "              OrderedDict([('precision', 0.0),\n",
       "                           ('recall', 0.0),\n",
       "                           ('f1', 0),\n",
       "                           ('n_predicted_entities', 1),\n",
       "                           ('n_true_entities', 7)]))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
    "eval_conll(model, test_tokens, test_tags, short_report=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Could we say that our model is state of the art and the results are acceptable for the task? Definately, we can say so. Nowadays, bidirectional RNNs are among of the state of the art approaches for solving NER problem, which outperform other classical methods such as classical CRFs for this task. Despite the fact that we used small training corpora (in comparison with usual sizes of corpora in Deep Learning), our results are quite good. In addition, in this task there are many possible named entities and for some of them we have only several dozens of trainig examples, which is definately small. However, even better results could be obtained by some combinations of several types of methods, e.g. see [this](https://arxiv.org/abs/1603.01354) paper if interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
