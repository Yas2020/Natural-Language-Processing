{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn to calculate with seq2seq model\n",
    "\n",
    "In this assignment, we will learn how to use neural networks to solve sequence-to-sequence prediction tasks. Seq2Seq models are very popular these days because they achieve great results in Machine Translation, Text Summarization, Conversational Modeling and more.\n",
    "\n",
    "Using sequence-to-sequence modeling, we are going to build a calculator for evaluating arithmetic expressions for addition and subtraction, by taking an equation as an input to the neural network and producing an answer as it's output.\n",
    "\n",
    "The resulting solution for this problem will be based on state-of-the-art approaches for sequence-to-sequence learning and we could easily adapt it to solve other tasks. However, for training a machine translation system or intellectual chat bot, it would be useful to have access to compute resources like GPU as training such systems is usually time consuming. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "For this task, we don't need to download any data â€” we will generate it on our own! We will use two operators (addition and subtraction) and work with positive integer numbers in some range. Here are examples of correct inputs and outputs:\n",
    "\n",
    "    Input: '1+2'\n",
    "    Output: '3'\n",
    "    \n",
    "    Input: '0-99'\n",
    "    Output: '-99'\n",
    "\n",
    "*Note, that there are no spaces between operators and operands.*\n",
    "\n",
    "\n",
    "Now we need to implement the function *generate_equations*, which will be used to generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allowed_operators = ['+', '-']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_equations(allowed_operators, dataset_size, min_value, max_value):\n",
    "    \"\"\"Generates pairs of equations and solutions to them.\n",
    "    \n",
    "       Each equation has a form of two integers with an operator in between.\n",
    "       Each solution is an integer with the result of the operaion.\n",
    "    \n",
    "        allowed_operators: list of strings, allowed operators.\n",
    "        dataset_size: an integer, number of equations to be generated.\n",
    "        min_value: an integer, min value of each operand.\n",
    "        max_value: an integer, max value of each operand.\n",
    "\n",
    "        result: a list of tuples of strings (equation, solution).\n",
    "    \"\"\"\n",
    "    sample = []\n",
    "    \n",
    "    rand = [(random.randint(min_value, max_value),random.randint(min_value, max_value)) for _ in range(dataset_size)]\n",
    "    equations = ['{}{}{}'.format(rand[i][0],random.choice(allowed_operators),rand[i][1]) for i in range(dataset_size)]\n",
    "    solutions = [str(eval(equation)) for equation in equations]\n",
    "    for equation, solution in zip(equations,solutions):\n",
    "        sample.append((equation,solution))\n",
    "        \n",
    "        \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the correctness of your implementation, use *test_generate_equations* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generate_equations():\n",
    "    allowed_operators = ['+', '-']\n",
    "    dataset_size = 10\n",
    "    for (input_, output_) in generate_equations(allowed_operators, dataset_size, 0, 100):\n",
    "        if not (type(input_) is str and type(output_) is str):\n",
    "            return \"Both parts should be strings.\"\n",
    "        if eval(input_) != int(output_):\n",
    "            return \"The (equation: {!r}, solution: {!r}) pair is incorrect.\".format(input_, output_)\n",
    "    return \"Tests passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_generate_equations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to generate the train and test data for the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_operators = ['+', '-']\n",
    "dataset_size = 100000\n",
    "data = generate_equations(allowed_operators, dataset_size, min_value=0, max_value=9999)\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for the neural network\n",
    "\n",
    "The next stage of data preparation is creating mappings of the characters to their indices in some vocabulary. Since in our task we already know which symbols will appear in the inputs and outputs, generating the vocabulary is a simple step.\n",
    "\n",
    "#### How to create dictionaries for other task\n",
    "\n",
    "First of all, we need to understand what is the basic unit of the sequence in the task. In our case, we operate on symbols and the basic unit is a symbol. The number of symbols is small, so we don't need to think about filtering/normalization steps. However, in other tasks, the basic unit is often a word, and in this case the mapping would be *word $\\to$ integer*. The number of words might be huge, so it would be reasonable to filter them, for example, by frequency and leave only the frequent ones. Other strategies that your should consider are: data normalization (lowercasing, tokenization, how to consider punctuation marks), separate vocabulary for input and for output (e.g. for machine translation), some specifics of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {symbol:i for i, symbol in enumerate('#^$+-1234567890')}\n",
    "id2word = {i:symbol for symbol, i in word2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_symbol = '^'\n",
    "end_symbol = '$'\n",
    "padding_symbol = '#'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could notice that we have added 3 special symbols: '^', '\\$' and '#':\n",
    "- '^' symbol will be passed to the network to indicate the beginning of the decoding procedure. We will discuss this one later in more details.\n",
    "- '\\$' symbol will be used to indicate the *end of a string*, both for input and output sequences. \n",
    "- '#' symbol will be used as a *padding* character to make lengths of all strings equal within one training batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When vocabularies are ready, we need to be able to convert a sentence to a list of vocabulary word indices and back. At the same time, let's care about padding. We are going to preprocess each sequence from the input (and output ground truth) in such a way that:\n",
    "- it has a predefined length *padded_len*\n",
    "- it is probably cut off or padded with the *padding symbol* '#'\n",
    "- it *always* ends with the *end symbol* '$'\n",
    "\n",
    "We will treat the original characters of the sequence **and the end symbol** as the valid part of the input. We will store *the actual length* of the sequence, which includes the end symbol, but does not include the padding symbols. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we need to implement the function *sentence_to_ids* that does the described job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_ids(sentence, word2id, padded_len):\n",
    "    \"\"\" Converts a sequence of symbols to a padded sequence of their ids.\n",
    "    \n",
    "      sentence: a string, input/output sequence of symbols.\n",
    "      word2id: a dict, a mapping from original symbols to ids.\n",
    "      padded_len: an integer, a desirable length of the sequence.\n",
    "\n",
    "      result: a tuple of (a list of ids, an actual length of sentence).\n",
    "    \"\"\"\n",
    "    \n",
    "    if padded_len <= len(sentence):\n",
    "        sentence = sentence[:padded_len-1] + '$'\n",
    "        sen_len = padded_len\n",
    "    else: \n",
    "        sen_len = len(sentence)+1 \n",
    "        sentence = sentence + '$' + (padded_len-len(sentence)-1)*'#'\n",
    "\n",
    "\n",
    "    sen_ids = [word2id[x] for x in sentence]\n",
    "    \n",
    "    return sen_ids, sen_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the implementation is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence_to_ids():\n",
    "    sentences = [(\"123+123\", 7), (\"123+123\", 8), (\"123+123\", 10)]\n",
    "    expected_output = [([5, 6, 7, 3, 5, 6, 2], 7), \n",
    "                       ([5, 6, 7, 3, 5, 6, 7, 2], 8), \n",
    "                       ([5, 6, 7, 3, 5, 6, 7, 2, 0, 0], 8)] \n",
    "    for (sentence, padded_len), (sentence_ids, expected_length) in zip(sentences, expected_output):\n",
    "        output, length = sentence_to_ids(sentence, word2id, padded_len)\n",
    "        if output != sentence_ids:\n",
    "            return(\"Convertion of '{}' for padded_len={} to {} is incorrect.\".format(\n",
    "                sentence, padded_len, output))\n",
    "        if length != expected_length:\n",
    "            return(\"Convertion of '{}' for padded_len={} has incorrect actual length {}.\".format(\n",
    "                sentence, padded_len, length))\n",
    "    return(\"Tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_sentence_to_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to be able to get back from indices to symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_sentence(ids, id2word):\n",
    "    \"\"\" Converts a sequence of ids to a sequence of symbols.\n",
    "    \n",
    "          ids: a list, indices for the padded sequence.\n",
    "          id2word:  a dict, a mapping from ids to original symbols.\n",
    "\n",
    "          result: a list of symbols.\n",
    "    \"\"\"\n",
    " \n",
    "    return [id2word[i] for i in ids] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of data preparation is a function that transforms a batch of sentences to a list of lists of indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_ids(sentences, word2id, max_len):\n",
    "    \"\"\"Prepares batches of indices. \n",
    "    \n",
    "       Sequences are padded to match the longest sequence in the batch,\n",
    "       if it's longer than max_len, then max_len is used instead.\n",
    "\n",
    "        sentences: a list (or tuple) of strings, original sequences.\n",
    "        word2id: a dict, a mapping from original symbols to ids.\n",
    "        max_len: an integer, max len of sequences allowed.\n",
    "\n",
    "        result: a list of lists of ids, a list of actual lengths.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_len_in_batch = min(max(len(s) for s in sentences) + 1, max_len)\n",
    "    batch_ids, batch_ids_len = [], []\n",
    "    for sentence in sentences:\n",
    "        ids, ids_len = sentence_to_ids(sentence, word2id, max_len_in_batch)\n",
    "        batch_ids.append(ids)\n",
    "        batch_ids_len.append(ids_len)\n",
    "    return batch_ids, batch_ids_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function *generate_batches* will help to generate batches with defined size from given samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(samples, batch_size=64):\n",
    "    X, Y = [], []\n",
    "    for i, (x, y) in enumerate(samples, 1):\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        if i % batch_size == 0:\n",
    "            yield X, Y\n",
    "            X, Y = [], []\n",
    "    if X and Y:\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the result of the implemented functions, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ('2743+1676', '4419')\n",
      "Ids: [[6, 11, 8, 7, 3, 5, 10, 11, 10, 2], [8, 8, 5, 13, 2, 0, 0, 0, 0, 0]]\n",
      "Sentences lengths: [10, 5]\n"
     ]
    }
   ],
   "source": [
    "sentences = train_set[0]\n",
    "ids, sent_lens = batch_to_ids(sentences, word2id, max_len=10)\n",
    "print('Input:', sentences)\n",
    "print('Ids: {}\\nSentences lengths: {}'.format(ids, sent_lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to see what generate_batches does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder architecture\n",
    "\n",
    "Encoder-Decoder is a successful architecture for Seq2Seq tasks with different lengths of input and output sequences. The main idea is to use two recurrent neural networks, where the first neural network *encodes* the input sequence into a real-valued vector and then the second neural network *decodes* this vector into the output sequence. While building the neural network, we will specify some particular characteristics of this architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use TensorFlow building blocks to specify the network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us specify the layers of the neural network. First, we need to prepare an embedding layer, which can be used for both  input and output. A random matrix followed by a lookup function will do as well. But embedding layers automatically pass the mask to the mask-consuming layers. So we dont use input lengths. In general, for tasks with different vocabularies there would be multiple embedding layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "The first RNN of the current architecture is called an *encoder* and serves for encoding an input sequence to a real-valued vector. Input of this RNN is an embedded input batch. Since sentences in the same batch could have different actual lengths, it is useful to provide input lengths to avoid unnecessary computations. This is done by masking produced by embedding layers. The final encoder state will be passed to the second RNN (decoder), which we will create soon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "\n",
    "The second RNN is called a *decoder* and serves for generating the output sequence. In the simple seq2seq arcitecture, the input sequence is provided to the decoder only as the final state of the encoder. Obviously, it is a bottleneck and [Attention techniques](https://www.tensorflow.org/tutorials/seq2seq#background_on_the_attention_mechanism) can help to overcome it, however we do not need them to make our calculator work though this would be a necessary ingredient for more advanced tasks. \n",
    "\n",
    "During training, decoder also uses information about the true output. It is feeded in as input symbol by symbol. However, during the prediction stage (which is called *inference* in this architecture), the decoder can only use its own generated output from the previous step to feed it in at the next step. Because of this difference (*training* vs *inference*), we will create two distinct instances, which will serve for the described scenarios.\n",
    "\n",
    "The picture below illustrates the point. It also shows our work with the special characters, e.g. look how the start symbol `^` is used. The transparent parts are ignored. In decoder, it is masked out in the loss computation. In encoder, the green state is considered as final and passed to the decoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"encoder-decoder-pic.png\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting some hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=15\n",
    "embeddings_size=20\n",
    "max_iter=7\n",
    "hidden_size=512\n",
    "start_id=1\n",
    "end_id=2\n",
    "padding_symbol_id =0\n",
    "dropout=0.5\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define encoder and decoder below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderNet(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, name='EncoderNet'):\n",
    "        \n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = L.Embedding(vocab_size,\n",
    "                                     embeddings_size,\n",
    "                                    ) \n",
    "        self.rnn = L.GRU(hidden_size, dropout=dropout)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to implement the decoder using BasicDecoder interface in TensorFLow. We also need a sampler for trainig and another for inference.\n",
    "\n",
    "As before, we should choose some RNN cell, e.g. GRU cell. To turn hidden states into logits, we will need a projection layer (dense without bias). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderNet(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = L.Embedding(vocab_size,embeddings_size)\n",
    "        self.dense = L.Dense(vocab_size)\n",
    "        self.grucell = L.GRUCell(hidden_size, dropout=dropout)\n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "        \n",
    "        # an instance of BaseDecoder\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(self.grucell, \n",
    "                                        sampler= self.sampler,\n",
    "                                        output_layer=self.dense,\n",
    "                                               )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we will use [sequence_loss](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/sequence_loss), which is a weighted cross-entropy loss for a sequence of logits. Also note, that we do not want to take into account loss terms coming from padding symbols, so we will mask them out using weights.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_pred, y, lengths):\n",
    "   \n",
    "    weights = tf.cast(tf.sequence_mask(lengths), dtype=tf.float32)\n",
    "    loss = tfa.seq2seq.sequence_loss(y_pred, y, weights)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_step(input_batch, ground_truth, ground_truth_lengths):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        input_batch_embed = encoder.embedding(input_batch)\n",
    "        final_state = encoder.rnn(input_batch_embed)\n",
    "\n",
    "        \n",
    "        batch_size = tf.shape(input_batch)[0]\n",
    "        start_tokens = tf.fill([batch_size], start_id)\n",
    "        ground_truth_as_input = tf.concat([tf.expand_dims(start_tokens, 1), ground_truth], 1)\n",
    "\n",
    "        \n",
    "        ground_truth_embedded = decoder.embedding(ground_truth_as_input) \n",
    "\n",
    "                \n",
    "        #BasicDecoderOutput \n",
    "        outputs, _, _  = decoder.decoder(ground_truth_embedded,\n",
    "                                        initial_state=[final_state],\n",
    "                                        )\n",
    "        \n",
    "        logits = outputs.rnn_output\n",
    "        \n",
    "        # dont predict the last token\n",
    "        loss = loss_function(logits[:,:-1,:], ground_truth, ground_truth_lengths)\n",
    "        \n",
    "        \n",
    "\n",
    "    #Returns the list of all layer variables / weights.\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables  \n",
    "    # differentiate loss wrt variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    #grads_and_vars â€“ List of(gradient, variable) pairs.\n",
    "    grads_and_vars = zip(gradients,variables)\n",
    "    optimizer.apply_gradients(grads_and_vars)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following produces predictions by the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(X_batch):\n",
    "    \n",
    "    X_batch, _ = batch_to_ids(X_batch, word2id, max_len)\n",
    "\n",
    "    X = tf.convert_to_tensor(X_batch, dtype=tf.int32)\n",
    "\n",
    "    encoder_initial_cell_state = [tf.zeros((batch_size, hidden_size))]\n",
    "\n",
    "    input_batch_embed = encoder.embedding(X)\n",
    "    final_state = encoder.rnn(input_batch_embed,\n",
    "                             initial_state =encoder_initial_cell_state)\n",
    "\n",
    "\n",
    "    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "\n",
    "    start_tokens = tf.fill([batch_size], start_id)\n",
    "    decoder_input = tf.expand_dims(start_tokens, 1)\n",
    "\n",
    "    decoder_emb_inp = decoder.embedding(decoder_input)\n",
    "\n",
    "    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.grucell, sampler = greedy_sampler,\n",
    "                                            output_layer=decoder.dense)\n",
    "\n",
    "    #initialize inference decoder\n",
    "    decoder_embedding_matrix = decoder.embedding.variables[0] \n",
    "    (first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
    "                             start_tokens = start_tokens,\n",
    "                             end_token=end_id,\n",
    "                             initial_state = [final_state])\n",
    "\n",
    "    inputs = first_inputs\n",
    "    state = first_state  \n",
    "    predictions = np.empty((batch_size,0), dtype = np.int32)                                                                             \n",
    "    for j in range(max_len*2):\n",
    "        outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
    "        inputs = next_inputs\n",
    "        state = next_state\n",
    "        outputs = np.expand_dims(outputs.sample_id,axis = -1)\n",
    "        predictions = np.append(predictions, outputs, axis = -1)\n",
    "\n",
    "    pred = []\n",
    "    for i in range(len(predictions)):\n",
    "        line = predictions[i,:]\n",
    "        seq = list(itertools.takewhile(lambda index: index !=2, line))\n",
    "        pred.append(\"\".join([id2word[w] for w in seq]))\n",
    "\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 \n",
    "max_len =20 \n",
    "\n",
    "n_step = int(len(train_set) / batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training... \n",
      "\n",
      "Train: epoch 1\n",
      "Epoch: [1/15], step: [1/625], loss: 2.708197, MAE for test: 7021.468750\n",
      "Epoch: [1/15], step: [201/625], loss: 1.870707, MAE for test: 4282.562500\n",
      "Epoch: [1/15], step: [401/625], loss: 1.782158, MAE for test: 3346.390625\n",
      "Epoch: [1/15], step: [601/625], loss: 1.634863, MAE for test: 1245.242188\n",
      "Train: epoch 2\n",
      "Epoch: [2/15], step: [1/625], loss: 1.599263, MAE for test: 1255.328125\n",
      "Epoch: [2/15], step: [201/625], loss: 1.482839, MAE for test: 540.695312\n",
      "Epoch: [2/15], step: [401/625], loss: 1.409111, MAE for test: 425.656250\n",
      "Epoch: [2/15], step: [601/625], loss: 1.364343, MAE for test: 333.226562\n",
      "Train: epoch 3\n",
      "Epoch: [3/15], step: [1/625], loss: 1.364885, MAE for test: 441.750000\n",
      "Epoch: [3/15], step: [201/625], loss: 1.316227, MAE for test: 333.515625\n",
      "Epoch: [3/15], step: [401/625], loss: 1.285151, MAE for test: 236.390625\n",
      "Epoch: [3/15], step: [601/625], loss: 1.260910, MAE for test: 182.859375\n",
      "Train: epoch 4\n",
      "Epoch: [4/15], step: [1/625], loss: 1.309271, MAE for test: 228.898438\n",
      "Epoch: [4/15], step: [201/625], loss: 1.286887, MAE for test: 189.750000\n",
      "Epoch: [4/15], step: [401/625], loss: 1.200383, MAE for test: 115.437500\n",
      "Epoch: [4/15], step: [601/625], loss: 1.205946, MAE for test: 104.171875\n",
      "Train: epoch 5\n",
      "Epoch: [5/15], step: [1/625], loss: 1.249182, MAE for test: 143.289062\n",
      "Epoch: [5/15], step: [201/625], loss: 1.138976, MAE for test: 109.046875\n",
      "Epoch: [5/15], step: [401/625], loss: 1.146014, MAE for test: 71.671875\n",
      "Epoch: [5/15], step: [601/625], loss: 1.163468, MAE for test: 86.367188\n",
      "Train: epoch 6\n",
      "Epoch: [6/15], step: [1/625], loss: 1.174022, MAE for test: 117.117188\n",
      "Epoch: [6/15], step: [201/625], loss: 1.131368, MAE for test: 119.695312\n",
      "Epoch: [6/15], step: [401/625], loss: 1.146451, MAE for test: 118.570312\n",
      "Epoch: [6/15], step: [601/625], loss: 1.100334, MAE for test: 101.679688\n",
      "Train: epoch 7\n",
      "Epoch: [7/15], step: [1/625], loss: 1.100537, MAE for test: 63.617188\n",
      "Epoch: [7/15], step: [201/625], loss: 1.078816, MAE for test: 66.289062\n",
      "Epoch: [7/15], step: [401/625], loss: 1.066973, MAE for test: 72.234375\n",
      "Epoch: [7/15], step: [601/625], loss: 1.107804, MAE for test: 76.531250\n",
      "Train: epoch 8\n",
      "Epoch: [8/15], step: [1/625], loss: 1.120832, MAE for test: 75.226562\n",
      "Epoch: [8/15], step: [201/625], loss: 1.121771, MAE for test: 71.796875\n",
      "Epoch: [8/15], step: [401/625], loss: 1.103546, MAE for test: 77.750000\n",
      "Epoch: [8/15], step: [601/625], loss: 1.057869, MAE for test: 56.804688\n",
      "Train: epoch 9\n",
      "Epoch: [9/15], step: [1/625], loss: 1.211044, MAE for test: 98.640625\n",
      "Epoch: [9/15], step: [201/625], loss: 1.001832, MAE for test: 53.679688\n",
      "Epoch: [9/15], step: [401/625], loss: 1.054400, MAE for test: 76.289062\n",
      "Epoch: [9/15], step: [601/625], loss: 1.093443, MAE for test: 128.328125\n",
      "Train: epoch 10\n",
      "Epoch: [10/15], step: [1/625], loss: 1.047804, MAE for test: 63.054688\n",
      "Epoch: [10/15], step: [201/625], loss: 1.043669, MAE for test: 62.070312\n",
      "Epoch: [10/15], step: [401/625], loss: 1.054103, MAE for test: 56.218750\n",
      "Epoch: [10/15], step: [601/625], loss: 1.047332, MAE for test: 59.125000\n",
      "Train: epoch 11\n",
      "Epoch: [11/15], step: [1/625], loss: 1.038096, MAE for test: 62.679688\n",
      "Epoch: [11/15], step: [201/625], loss: 1.068980, MAE for test: 54.914062\n",
      "Epoch: [11/15], step: [401/625], loss: 1.033142, MAE for test: 64.148438\n",
      "Epoch: [11/15], step: [601/625], loss: 1.016420, MAE for test: 42.132812\n",
      "Train: epoch 12\n",
      "Epoch: [12/15], step: [1/625], loss: 1.058590, MAE for test: 54.765625\n",
      "Epoch: [12/15], step: [201/625], loss: 1.006457, MAE for test: 42.843750\n",
      "Epoch: [12/15], step: [401/625], loss: 1.045373, MAE for test: 51.242188\n",
      "Epoch: [12/15], step: [601/625], loss: 1.018109, MAE for test: 49.367188\n",
      "Train: epoch 13\n",
      "Epoch: [13/15], step: [1/625], loss: 1.033597, MAE for test: 45.320312\n",
      "Epoch: [13/15], step: [201/625], loss: 1.028178, MAE for test: 52.117188\n",
      "Epoch: [13/15], step: [401/625], loss: 0.977720, MAE for test: 41.656250\n",
      "Epoch: [13/15], step: [601/625], loss: 0.983927, MAE for test: 46.593750\n",
      "Train: epoch 14\n",
      "Epoch: [14/15], step: [1/625], loss: 0.974901, MAE for test: 38.085938\n",
      "Epoch: [14/15], step: [201/625], loss: 0.964517, MAE for test: 39.468750\n",
      "Epoch: [14/15], step: [401/625], loss: 0.992882, MAE for test: 42.656250\n",
      "Epoch: [14/15], step: [601/625], loss: 0.976117, MAE for test: 34.820312\n",
      "Train: epoch 15\n",
      "Epoch: [15/15], step: [1/625], loss: 1.000024, MAE for test: 42.296875\n",
      "Epoch: [15/15], step: [201/625], loss: 1.034047, MAE for test: 54.726562\n",
      "Epoch: [15/15], step: [401/625], loss: 1.009403, MAE for test: 46.507812\n",
      "Epoch: [15/15], step: [601/625], loss: 1.000188, MAE for test: 69.875000\n"
     ]
    }
   ],
   "source": [
    "invalid_number_prediction_counts = []\n",
    "all_model_predictions = []\n",
    "all_ground_truth = []\n",
    "\n",
    "\n",
    "n_epochs = 15 \n",
    "\n",
    "\n",
    "\n",
    "print('Start training... \\n')\n",
    "for epoch in range(n_epochs):  \n",
    "    np.random.shuffle(train_set)\n",
    "    np.random.shuffle(test_set)\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    print('Train: epoch', epoch + 1)\n",
    "    for n_iter, (X_batch, Y_batch) in enumerate(generate_batches(train_set, batch_size=batch_size)):\n",
    "        \n",
    "        \n",
    "        X,X_seq_len = batch_to_ids(X_batch, word2id, max_len)\n",
    "        Y,Y_seq_len = batch_to_ids(Y_batch, word2id, max_len)\n",
    "                \n",
    "        X = tf.convert_to_tensor(X, dtype=tf.int32)\n",
    "        Y = tf.convert_to_tensor(Y, dtype=tf.int32)\n",
    "        Y_seq_len = tf.convert_to_tensor(Y_seq_len, dtype=tf.int32)\n",
    "        \n",
    "        batch_loss = tf.keras.backend.eval(train_step(X, Y, Y_seq_len))\n",
    "        loss += batch_loss\n",
    "        \n",
    "        mae = []\n",
    "        \n",
    "        if n_iter % 200 == 0:\n",
    "            X_test_batch, Y_test_batch = next(generate_batches(test_set, batch_size=batch_size))\n",
    "            if len(X_test_batch) == batch_size:\n",
    "                pred = predictions(X_test_batch)\n",
    "                pred = [int(i) for i in pred]\n",
    "                true = [int(i) for i in Y_test_batch]\n",
    "                mae.append(mean_absolute_error(true, pred))\n",
    "\n",
    "\n",
    "        \n",
    "            print(\"Epoch: [%d/%d], step: [%d/%d], loss: %f, MAE for test: %f\" % (epoch + 1, n_epochs, n_iter + 1, \n",
    "                                                               n_step, batch_loss,sum(mae)/len(mae)\n",
    "                                                              ))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
